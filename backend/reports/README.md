# Reports Directory

This directory contains all evaluation reports, model metrics, and visualizations generated by the Floodingnaque flood prediction system.

## Directory Structure

```
reports/
├── README.md                    # This file
├── evaluation/                  # Model evaluation reports
│   └── evaluation_report_YYYYMMDD_HHMMSS.json
├── v{N}/                        # Version-specific reports (v5, v6, etc.)
│   ├── confusion_matrix.png
│   ├── feature_importance.png
│   ├── learning_curves.png
│   ├── metrics_comparison.png
│   ├── model_report.txt
│   ├── precision_recall_curve.png
│   ├── robustness_report.json
│   └── roc_curve.png
├── archive/                     # Historical versioned reports (v1-v4)
├── model_report.txt             # Current model's detailed report
├── pipeline_summary.json        # Training pipeline execution summary
├── evaluation_report.json       # Quick model evaluation results
├── validation.json              # Data validation results
├── model_comparison.csv         # Multi-model comparison metrics
├── comparison_report.txt        # Textual comparison analysis
└── *.png                        # Current model visualizations
```

## Report Types

### Model Reports (`model_report.txt`, `v{N}/model_report.txt`)
Comprehensive training report including:
- Model version and type
- Training data summary
- Performance metrics (accuracy, precision, recall, F1)
- Confusion matrix
- Feature importance rankings

### Evaluation Reports (`evaluation_report*.json`)
System-level evaluation aligned with research objectives:
- API integration status
- Scalability metrics
- Reliability metrics
- Usability assessment

**Naming Convention:** `evaluation_report_YYYYMMDD_HHMMSS.json`

### Robustness Reports (`robustness_report.json`)
Model robustness testing results:
- Cross-validation scores
- Feature perturbation analysis
- Edge case handling

### Pipeline Summary (`pipeline_summary.json`)
Training pipeline execution metadata:
- Execution status
- Duration
- Warnings/errors

### Visualizations
| File | Description |
|------|-------------|
| `confusion_matrix.png` | True vs predicted classification matrix |
| `feature_importance.png` | Bar chart of feature importance scores |
| `learning_curves.png` | Training/validation score vs dataset size |
| `roc_curve.png` | Receiver Operating Characteristic curve |
| `precision_recall_curve.png` | Precision-recall tradeoff curve |
| `metrics_comparison.png` | Cross-version metrics comparison |
| `metrics_evolution.png` | Metrics changes over model versions |
| `model_progression_chart.png` | Version-by-version improvement chart |
| `parameters_evolution.png` | Hyperparameter changes over versions |

## Usage

### Generate Reports
Reports are automatically generated during model training:
```powershell
cd backend
python scripts/train_model.py --generate-reports
```

### View Current Metrics
```powershell
python -c "import json; print(json.dumps(json.load(open('reports/evaluation_report.json')), indent=2))"
```

## Migration Notes

**As of January 2026:** The `evaluation_results/` directory has been consolidated into this `reports/` directory. Historical evaluation reports can be found in `reports/evaluation/`.
