"""
Performance Benchmark Tests.

Tests for response time assertions and memory usage.
"""

import gc
import sys
import time
from unittest.mock import MagicMock, patch

import pytest

# ============================================================================
# Response Time Benchmark Tests
# ============================================================================


class TestResponseTimeBenchmarks:
    """Performance benchmarks for API response times."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_health_endpoint_response_time(self, client):
        """Health endpoint should respond within 100ms."""
        start = time.perf_counter()
        response = client.get("/health")
        elapsed = time.perf_counter() - start

        assert response.status_code == 200
        assert elapsed < 0.1, f"Health check took {elapsed:.3f}s, expected < 0.1s"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_status_endpoint_response_time(self, client):
        """Status endpoint should respond within 100ms."""
        start = time.perf_counter()
        response = client.get("/status")
        elapsed = time.perf_counter() - start

        assert response.status_code == 200
        assert elapsed < 0.1, f"Status check took {elapsed:.3f}s, expected < 0.1s"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_root_endpoint_response_time(self, client):
        """Root endpoint should respond within 100ms."""
        start = time.perf_counter()
        response = client.get("/")
        elapsed = time.perf_counter() - start

        assert response.status_code == 200
        assert elapsed < 0.1, f"Root endpoint took {elapsed:.3f}s, expected < 0.1s"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    @patch("app.services.predict.load_model")
    def test_prediction_endpoint_response_time(self, mock_load, client, api_headers):
        """Prediction endpoint should respond within 500ms."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [[0]]
        mock_model.predict_proba.return_value = [[0.8, 0.2]]
        mock_load.return_value = mock_model

        payload = {"temperature": 298.15, "humidity": 75.0, "precipitation": 5.0}

        start = time.perf_counter()
        response = client.post("/api/v1/predict", json=payload, headers=api_headers)
        elapsed = time.perf_counter() - start

        assert elapsed < 0.5, f"Prediction took {elapsed:.3f}s, expected < 0.5s"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_batch_health_checks_throughput(self, client):
        """Health endpoint should handle high throughput."""
        num_requests = 50

        start = time.perf_counter()
        for _ in range(num_requests):
            response = client.get("/health")
            assert response.status_code == 200
        elapsed = time.perf_counter() - start

        requests_per_second = num_requests / elapsed

        # Should handle at least 100 requests per second
        assert requests_per_second >= 50, f"Throughput {requests_per_second:.1f} req/s, expected >= 50"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_average_response_time(self, client):
        """Calculate average response time over multiple requests."""
        num_requests = 20
        times = []

        for _ in range(num_requests):
            start = time.perf_counter()
            response = client.get("/health")
            elapsed = time.perf_counter() - start
            times.append(elapsed)
            assert response.status_code == 200

        avg_time = sum(times) / len(times)
        max_time = max(times)
        min_time = min(times)

        # Average should be under 50ms
        assert avg_time < 0.05, f"Avg response time {avg_time*1000:.1f}ms, expected < 50ms"

        # Max time should not be too far from average (no huge outliers)
        assert max_time < avg_time * 5, f"Max time {max_time*1000:.1f}ms is >5x avg"


# ============================================================================
# Memory Usage Benchmark Tests
# ============================================================================


class TestMemoryBenchmarks:
    """Performance benchmarks for memory usage."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_request_memory_footprint(self, client):
        """Single request should not cause memory leak."""
        gc.collect()
        initial_memory = self._get_memory_usage()

        # Make several requests
        for _ in range(100):
            response = client.get("/health")
            assert response.status_code == 200

        gc.collect()
        final_memory = self._get_memory_usage()

        # Memory increase should be minimal (less than 5MB)
        memory_increase = final_memory - initial_memory
        assert memory_increase < 5 * 1024 * 1024, f"Memory increased by {memory_increase / 1024 / 1024:.2f}MB"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    @patch("app.services.predict.load_model")
    def test_prediction_memory_footprint(self, mock_load, client, api_headers):
        """Prediction requests should not cause memory leak."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [[0]]
        mock_model.predict_proba.return_value = [[0.8, 0.2]]
        mock_load.return_value = mock_model

        gc.collect()
        initial_memory = self._get_memory_usage()

        payload = {"temperature": 298.15, "humidity": 75.0, "precipitation": 5.0}

        # Make several prediction requests
        for _ in range(50):
            response = client.post("/api/v1/predict", json=payload, headers=api_headers)

        gc.collect()
        final_memory = self._get_memory_usage()

        # Memory increase should be minimal (less than 10MB)
        memory_increase = final_memory - initial_memory
        assert memory_increase < 10 * 1024 * 1024, f"Memory increased by {memory_increase / 1024 / 1024:.2f}MB"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_large_payload_memory(self, client, api_headers):
        """Large payloads should be handled without excessive memory."""
        gc.collect()
        initial_memory = self._get_memory_usage()

        # Create a large payload
        large_payload = {"data": [{"value": i, "name": f"item_{i}"} for i in range(1000)]}

        response = client.post("/api/v1/predict", json=large_payload, headers=api_headers)

        gc.collect()
        final_memory = self._get_memory_usage()

        # Memory increase should be reasonable
        memory_increase = final_memory - initial_memory
        assert memory_increase < 20 * 1024 * 1024, f"Memory increased by {memory_increase / 1024 / 1024:.2f}MB"

    def _get_memory_usage(self):
        """Get current process memory usage."""
        try:
            import psutil

            process = psutil.Process()
            return process.memory_info().rss
        except ImportError:
            # Fallback to sys.getsizeof for objects
            return 0  # Cannot measure without psutil


# ============================================================================
# Concurrent Load Benchmark Tests
# ============================================================================


class TestConcurrentLoadBenchmarks:
    """Performance benchmarks for concurrent load handling."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    def test_concurrent_requests_handling(self, client):
        """Test handling of concurrent requests."""
        import concurrent.futures

        num_requests = 20

        def make_request():
            start = time.perf_counter()
            response = client.get("/health")
            elapsed = time.perf_counter() - start
            return response.status_code, elapsed

        start = time.perf_counter()

        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]

        total_time = time.perf_counter() - start

        # All requests should succeed
        status_codes = [r[0] for r in results]
        assert all(s == 200 for s in status_codes), f"Some requests failed: {status_codes}"

        # Total time should be less than sequential time
        avg_request_time = sum(r[1] for r in results) / len(results)
        assert total_time < num_requests * avg_request_time, "Concurrent requests slower than sequential"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    def test_sustained_load(self, client):
        """Test sustained load over time."""
        duration_seconds = 2
        request_count = 0
        error_count = 0

        start = time.perf_counter()

        while time.perf_counter() - start < duration_seconds:
            response = client.get("/health")
            request_count += 1
            if response.status_code != 200:
                error_count += 1

        elapsed = time.perf_counter() - start
        requests_per_second = request_count / elapsed
        error_rate = error_count / request_count if request_count > 0 else 0

        # Error rate should be very low
        assert error_rate < 0.01, f"Error rate {error_rate*100:.1f}% is too high"

        # Should handle reasonable throughput
        assert requests_per_second >= 10, f"Only {requests_per_second:.1f} req/s under sustained load"


# ============================================================================
# Response Size Benchmark Tests
# ============================================================================


class TestResponseSizeBenchmarks:
    """Performance benchmarks for response sizes."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_health_response_size(self, client):
        """Health response should be small."""
        response = client.get("/health")

        response_size = len(response.data)

        # Health response should be under 1KB
        assert response_size < 1024, f"Health response {response_size} bytes, expected < 1KB"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_error_response_size(self, client):
        """Error responses should be reasonably sized."""
        response = client.get("/nonexistent-endpoint")

        response_size = len(response.data)

        # Error response should be under 2KB
        assert response_size < 2048, f"Error response {response_size} bytes, expected < 2KB"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @patch("app.services.predict.load_model")
    def test_prediction_response_size(self, mock_load, client, api_headers):
        """Prediction response should be reasonably sized."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [[0]]
        mock_model.predict_proba.return_value = [[0.8, 0.2]]
        mock_load.return_value = mock_model

        payload = {"temperature": 298.15, "humidity": 75.0, "precipitation": 5.0}

        response = client.post("/api/v1/predict", json=payload, headers=api_headers)

        if response.status_code in [200, 201]:
            response_size = len(response.data)

            # Prediction response should be under 5KB
            assert response_size < 5120, f"Prediction response {response_size} bytes, expected < 5KB"


# ============================================================================
# Database Query Benchmark Tests
# ============================================================================


class TestDatabaseQueryBenchmarks:
    """Performance benchmarks for database queries."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_simple_query_time(self, db_session):
        """Simple queries should be fast."""
        from sqlalchemy import text

        start = time.perf_counter()
        result = db_session.execute(text("SELECT 1"))
        result.fetchall()
        elapsed = time.perf_counter() - start

        # Simple query should be under 10ms
        assert elapsed < 0.01, f"Simple query took {elapsed*1000:.1f}ms, expected < 10ms"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    def test_batch_query_time(self, db_session):
        """Batch queries should complete in reasonable time."""
        from sqlalchemy import text

        num_queries = 100

        start = time.perf_counter()
        for _ in range(num_queries):
            result = db_session.execute(text("SELECT 1"))
            result.fetchall()
        elapsed = time.perf_counter() - start

        avg_time = elapsed / num_queries

        # Average query time should be under 5ms
        assert avg_time < 0.005, f"Avg query time {avg_time*1000:.1f}ms, expected < 5ms"

    @pytest.mark.performance
    @pytest.mark.benchmark
    def test_connection_pool_efficiency(self, db_session):
        """Connection pool should provide efficient connections."""
        from sqlalchemy import text

        # Multiple transactions should reuse connections efficiently
        start = time.perf_counter()

        for _ in range(20):
            db_session.execute(text("SELECT 1"))
            db_session.commit()

        elapsed = time.perf_counter() - start

        # All transactions should complete quickly
        assert elapsed < 1.0, f"Connection pool test took {elapsed:.2f}s, expected < 1s"


# ============================================================================
# Model Inference Benchmark Tests
# ============================================================================


class TestModelInferenceBenchmarks:
    """Performance benchmarks for model inference."""

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    def test_model_load_time(self):
        """Model should load within acceptable time."""
        try:
            from app.services.predict import _load_model

            start = time.perf_counter()
            model = _load_model()
            elapsed = time.perf_counter() - start

            # Model load should be under 5 seconds
            assert elapsed < 5.0, f"Model load took {elapsed:.2f}s, expected < 5s"
        except (ImportError, FileNotFoundError, Exception):
            pytest.skip("Model loading not available")

    @pytest.mark.performance
    @pytest.mark.benchmark
    @patch("app.services.predict.load_model")
    def test_inference_latency(self, mock_load):
        """Single inference should be fast."""
        import numpy as np

        mock_model = MagicMock()
        mock_model.predict.return_value = [[0]]
        mock_model.predict_proba.return_value = [[0.8, 0.2]]
        mock_load.return_value = mock_model

        model = mock_load()

        features = np.array([[298.15, 75.0, 5.0]])

        start = time.perf_counter()
        prediction = model.predict(features)
        elapsed = time.perf_counter() - start

        # Single inference should be under 100ms
        assert elapsed < 0.1, f"Inference took {elapsed*1000:.1f}ms, expected < 100ms"

    @pytest.mark.performance
    @pytest.mark.benchmark
    @pytest.mark.slow
    @patch("app.services.predict.load_model")
    def test_batch_inference_throughput(self, mock_load):
        """Batch inference should have good throughput."""
        import numpy as np

        mock_model = MagicMock()
        mock_model.predict.return_value = [[0]] * 100
        mock_model.predict_proba.return_value = [[0.8, 0.2]] * 100
        mock_load.return_value = mock_model

        model = mock_load()

        # Batch of 100 samples
        features = np.array([[298.15 + i, 75.0, 5.0] for i in range(100)])

        start = time.perf_counter()
        predictions = model.predict(features)
        elapsed = time.perf_counter() - start

        throughput = 100 / elapsed

        # Should process at least 1000 samples per second
        assert throughput >= 100, f"Batch throughput {throughput:.0f}/s, expected >= 100"
