# =====================================================
# FLOODINGNAQUE BACKEND - ENVIRONMENT CONFIGURATION TEMPLATE
# =====================================================
# 
# USAGE:
#   1. Copy this file to .env.development, .env.staging, or .env.production
#   2. Set APP_ENV to select which file to load:
#      - APP_ENV=development -> loads .env.development
#      - APP_ENV=staging     -> loads .env.staging
#      - APP_ENV=production  -> loads .env.production
#   3. Fill in your environment-specific values
#
# SECURITY:
#   - NEVER commit real API keys or secrets to git
#   - Use separate keys for each environment
#   - Rotate keys immediately if exposed
#

# ====================
# API Keys (REQUIRED)
# ====================
# Get your free API key from: https://openweathermap.org/api
OWM_API_KEY=your_openweathermap_api_key_here
OWM_UNITS=metric
OWM_LANG=en

# Get your free API key from: https://weatherstack.com/
WEATHERSTACK_API_KEY=your_weatherstack_api_key_here

# Note: Meteostat is a Python library and does not require an API key
# It provides historical weather data from public weather stations

# Internal API auth token for microservices 
INTERNAL_API_TOKEN=your_internal_api_token_here

# ====================
# APP Environment
# ====================
APP_ENV=production
ENABLE_HTTPS=True

# ====================
# Database Configuration
# ====================
# SQLite (default for development)
# Format: sqlite:///path/to/file.db
# DATABASE_URL=sqlite:///data/floodingnaque.db

# PostgreSQL (recommended for production)
# DATABASE_URL=postgresql://username:password@localhost:5432/floodingnaque

# Supabase PostgreSQL (RECOMMENDED - cloud hosted)
# Get your connection string from: https://supabase.com/dashboard/project/YOUR_PROJECT/settings/database
# IMPORTANT: Always use ?sslmode=require for production security
SUPABASE_URL=https://your-project-ref.supabase.co
SUPABASE_KEY=your_supabase_anon_key_here
SUPABASE_SECRET_KEY=your_supabase_service_role_key_here
# Connection string format: postgresql://postgres.[project-ref]:[password]@aws-0-[region].pooler.supabase.com:6543/postgres?sslmode=require
DATABASE_URL=postgresql://postgres:your_password@db.your-project-ref.supabase.co:5432/postgres?sslmode=require

# ====================
# Database SSL Configuration
# ====================
# SSL Mode Options:
#   - require (default for development): Encrypted connection, no certificate verification
#   - verify-ca: Encrypted + verify CA certificate
#   - verify-full (default for production/staging): Encrypted + verify CA + hostname check
#
# Environment defaults (if DB_SSL_MODE not set):
#   - Development: require (no certificate needed)
#   - Staging/Production: verify-full (certificate required, fail-fast if missing)
#
# For verify-ca or verify-full, you MUST set DB_SSL_CA_CERT to the CA certificate path
# The certificate file is included at: ./prod-ca-2021.crt (Supabase Root CA)
#
# Example configurations:
#   Development:  DB_SSL_MODE=require (no cert needed, encrypted connection)
#   Staging:      DB_SSL_MODE=verify-full + DB_SSL_CA_CERT=/app/certs/prod-ca-2021.crt
#   Production:   DB_SSL_MODE=verify-full + DB_SSL_CA_CERT=/app/certs/prod-ca-2021.crt
#
DB_SSL_MODE=require
DB_SSL_CA_CERT=

# MySQL (alternative)
# DATABASE_URL=mysql+pymysql://username:password@localhost:3306/floodingnaque

# ====================
# Server Configuration
# ====================
PORT=5000
HOST=0.0.0.0
FLASK_DEBUG=False
# FLASK_ENV is deprecated in Flask 2.3+ - use FLASK_DEBUG instead

# ====================
# Security Settings
# ====================
SECRET_KEY=change_this_to_a_random_secret_key_in_production
JWT_SECRET_KEY=change_this_to_another_random_secret_key

# API Key Authentication
# Comma-separated list of valid API keys for protecting sensitive endpoints
# Generate secure keys using: python -c "import secrets; print(secrets.token_urlsafe(32))"
VALID_API_KEYS=your_api_key_1,your_api_key_2

# Enable auth bypass in development (NEVER in production!)
AUTH_BYPASS_ENABLED=False

# API Key Security
# Minimum entropy (randomness) required for API keys (bits per character)
# Higher values = more secure but stricter validation
API_KEY_MIN_ENTROPY=3.0
# Default expiration for API keys (0 = no expiration)
API_KEY_DEFAULT_EXPIRY_DAYS=0

# Brute Force Protection
# Lock out IP after this many failed authentication attempts
AUTH_MAX_FAILED_ATTEMPTS=5
# Time window for counting failed attempts (seconds)
AUTH_FAILED_ATTEMPT_WINDOW=300
# How long to lock out an IP after max failures (seconds)
AUTH_LOCKOUT_DURATION=900

# Request Size Limits (in MB)
MAX_CONTENT_LENGTH_MB=1

# Model Integrity and Security
VERIFY_MODEL_INTEGRITY=True
REQUIRE_MODEL_SIGNATURE=False
# Generate with: python -c "import secrets; print(secrets.token_hex(32))"
MODEL_SIGNING_KEY=

# ====================
# Alert System 
# ====================
# SMS Gateway Configuration
# Supported providers: semaphore (Philippines), twilio (International)
SMS_ENABLED=False
SMS_SANDBOX_MODE=True
SMS_PROVIDER=semaphore

# Semaphore SMS (Philippines - Affordable) - https://semaphore.co/
SEMAPHORE_API_KEY=
SEMAPHORE_SENDER_NAME=FloodAlert

# Twilio SMS (International) - https://twilio.com/
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=
TWILIO_FROM_NUMBER=

# Email Configuration (SMTP)
EMAIL_ENABLED=False
EMAIL_SANDBOX_MODE=True
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=
SMTP_PASSWORD=
SMTP_FROM_EMAIL=
SMTP_USE_TLS=True

# ====================
# Logging Configuration
# ====================
LOG_LEVEL=INFO
LOG_FILE=logs/floodingnaque.log
LOG_FORMAT=json
LOG_MAX_SIZE_MB=10
LOG_BACKUP_COUNT=5
LOG_COMPRESS=True

# ====================
# Rate Limiting
# ====================
RATE_LIMIT_ENABLED=True
RATE_LIMIT_DEFAULT=100
RATE_LIMIT_WINDOW_SECONDS=3600
# Use redis:// for production (recommended), memory:// for development
RATE_LIMIT_STORAGE_URL=memory://
# Example Redis: redis://localhost:6379/0

# Burst Allowance Configuration
# Allows temporary spikes in traffic above normal rate limits
RATE_LIMIT_BURST_ENABLED=True
RATE_LIMIT_BURST_MULTIPLIER=2.0
RATE_LIMIT_BURST_WINDOW=10

# IP Reputation System for Rate Limiting
# Comma-separated list of IPs to always allow (bypass reputation checks)
IP_WHITELIST=
# Comma-separated list of IPs to always block
IP_BLACKLIST=

# API Key Tier Mapping (optional)
# Format: key_hash1:tier1,key_hash2:tier2
# Tiers: free, basic, pro, enterprise, unlimited
API_KEY_TIERS=

# ====================
# Parañaque City Default Coordinates
# ====================
DEFAULT_LATITUDE=14.4793
DEFAULT_LONGITUDE=121.0198
DEFAULT_LOCATION=Parañaque City

# ====================
# Model Configuration
# ====================
MODEL_DIR=models
MODEL_NAME=flood_rf_model
MODEL_VERSION=1.0
MODEL_RETRAIN_CRON=0 0 * * *
AUTO_RETRAIN=False
RETRAIN_INTERVAL_DAYS=30

# Training Resource Allocation
# TRAINING_N_JOBS: Explicit number of CPU cores for parallel training
#   - Leave blank for auto-detection based on DB_POOL_SIZE
#   - Set to specific number if you know your server capacity
#   - Example: TRAINING_N_JOBS=4 for 4-core training
# The training script will:
#   1. Use TRAINING_N_JOBS if set
#   2. Otherwise use DB_POOL_SIZE/2 if DB_POOL_SIZE < 20
#   3. Otherwise use (CPU_COUNT - 1)
TRAINING_N_JOBS=

# ====================
# Data Retention
# ====================
WEATHER_DATA_RETENTION_DAYS=365
PREDICTION_DATA_RETENTION_DAYS=180
ALERT_HISTORY_RETENTION_DAYS=90

# ====================
# Performance & Database Pool
# ====================
# Connection Pool Settings:
# - Supabase Free: pool_size=3, max_overflow=5
# - Supabase Pro: pool_size=20, max_overflow=10
# - Self-hosted: pool_size=20-50 based on resources
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=10
DB_POOL_RECYCLE=1800
DB_POOL_TIMEOUT=30
DB_POOL_PRE_PING=True
DB_ECHO_POOL=False

# Slow Query Logging
# Queries taking longer than this threshold (ms) are logged
SLOW_QUERY_THRESHOLD_MS=100

# Response Time Tracking
RESPONSE_TIME_TRACKING_ENABLED=True

# Query Cache Settings
DATA_CACHE_TTL=60
PREDICTION_CACHE_TTL=300
PREDICTION_CACHE_ENABLED=True

# Redis for Rate Limiting, Caching, and Sessions (recommended for production)
# SECURITY: Always use rediss:// (TLS) with authentication for production
# Format: redis://[[username]:[password]]@localhost:6379/0
# TLS Format: rediss://[[username]:[password]]@host:6380/0 (note: rediss:// not redis://)
REDIS_URL=
# For Redis Cloud or production, use:
# REDIS_URL=rediss://default:your_password@your-redis-host.redis.cache.windows.net:6380/0

# Rate Limit Storage (uses Redis if REDIS_URL is set)
RATE_LIMIT_STORAGE_URL=memory://

# Simple cache settings (when Redis not available)
CACHE_ENABLED=False
CACHE_TYPE=simple
CACHE_DEFAULT_TIMEOUT=300

# ====================
# CORS Configuration
# ====================
CORS_ORIGINS=https://floodingnaque.vercel.app
CORS_METHODS=GET,POST,PUT,DELETE,PATCH,OPTIONS
CORS_ALLOW_HEADERS=Content-Type,Authorization,X-API-Key,X-Request-ID,X-CSRF-Token,Accept,Origin

# ====================
# Scheduler Configuration
# ====================
SCHEDULER_ENABLED=True
SCHEDULER_TIMEZONE=Asia/Manila
DATA_INGEST_INTERVAL_HOURS=1
MODEL_EVALUATION_INTERVAL_DAYS=7
DATABASE_CLEANUP_INTERVAL_DAYS=1

# ====================
# Data Directory Pointers
# ====================
# Avoid hard-coding paths - configure directories here
DATA_DIR=data
LOGS_DIR=logs
UPLOADS_DIR=uploads
TEMP_DIR=temp
BACKUP_DIR=backups

# ====================
# Security Headers
# ====================
# HTTPS/HSTS Settings (enforced by security middleware)
HSTS_MAX_AGE=31536000
HSTS_INCLUDE_SUBDOMAINS=True
# IMPORTANT: Set to True in production AFTER testing, then submit to hstspreload.org
HSTS_PRELOAD=True

# Cross-Origin Policies (for APIs)
# Note: These are configured but may need implementation in security middleware
# COEP = Cross-Origin-Embedder-Policy
# COOP = Cross-Origin-Opener-Policy  
# CORP = Cross-Origin-Resource-Policy
COEP_POLICY=unsafe-none
COOP_POLICY=same-origin-allow-popups
CORP_POLICY=cross-origin

# Content Security Policy (customize for your frontend)
# Uncomment and customize if you need CSP headers
# CSP_POLICY=default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'

# CSP Reporting (recommended for monitoring policy violations)
# Option 1: Use your self-hosted endpoint (recommended)
CSP_REPORT_URI=/csp-report
CSP_REPORT_LOG_FILE=logs/csp_violations.log

# Option 2: Use external service like Report URI
# CSP_REPORT_URI=https://your-subdomain.report-uri.com/r/d/csp/enforce

# For Report-To header (newer browsers)
# CSP_REPORT_TO={"group":"csp-endpoint","max_age":10886400,"endpoints":[{"url":"/csp-report/report-to"}]}

# ====================
# Security.txt Configuration (RFC 9116)
# ====================
# These values configure the /.well-known/security.txt endpoint
# for security researchers to contact you about vulnerabilities
SECURITY_CONTACT_EMAIL=security@floodingnaque.com
SECURITY_POLICY_URL=https://floodingnaque.com/security-policy
SECURITY_ACKNOWLEDGMENTS_URL=https://floodingnaque.com/security/thanks
SECURITY_CANONICAL_URL=https://api.floodingnaque.com/.well-known/security.txt
# Expiration date for security.txt (ISO 8601 format)
# IMPORTANT: Update this before it expires!
SECURITY_TXT_EXPIRES=2026-12-31T23:59:59Z

# ====================
# Encryption Settings
# ====================
# Data encryption at rest
ENCRYPTION_ENABLED=True
ENCRYPTION_KEY=generate_a_64_character_hex_key_using_secrets_module
ENCRYPTION_ALGORITHM=AES-256-GCM
# Token encryption
TOKEN_ENCRYPTION_KEY=generate_another_64_character_hex_key_using_secrets_module
TOKEN_EXPIRY_HOURS=24

# ====================
# IP Whitelist (Admin Dashboard)
# ====================
# Comma-separated list of allowed IPs for admin access
IP_WHITELIST_ENABLED=True
ADMIN_ALLOWED_IPS=127.0.0.1,::1
ADMIN_DASHBOARD_ENABLED=True
ADMIN_REQUIRE_2FA=True

# ====================
# Feature Toggles
# ====================
# Enable/disable features without code changes
FEATURE_PREDICTIONS_ENABLED=True
FEATURE_ALERTS_ENABLED=True
FEATURE_HISTORICAL_DATA_ENABLED=True
FEATURE_REAL_TIME_WEATHER_ENABLED=True
FEATURE_MODEL_RETRAINING_ENABLED=False
FEATURE_API_DOCS_ENABLED=True
FEATURE_ADMIN_DASHBOARD_ENABLED=True
FEATURE_EXPORT_DATA_ENABLED=True

# ====================
# Observability & Telemetry
# ====================
# Application Performance Monitoring (APM)
TELEMETRY_ENABLED=True
APM_SERVICE_NAME=floodingnaque-backend
APM_ENVIRONMENT=production

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_TRACES_ENABLED=False
OTEL_METRICS_ENABLED=False
OTEL_LOGS_ENABLED=False

# Sentry Error Tracking (RECOMMENDED for production)
# Get your DSN from: https://sentry.io/settings/projects/
# Format: https://<key>@<organization>.ingest.sentry.io/<project-id>
# PRODUCTION DSN (configured for Floodingnaque):
# SENTRY_DSN=https://9967b23424ad6c700aedaba31a0c88ea@o4510565898649601.ingest.us.sentry.io/4510679270227968
# Uncomment above and remove the empty SENTRY_DSN below when deploying to production
SENTRY_DSN=
SENTRY_ENVIRONMENT=production
SENTRY_RELEASE=2.0.0

# Performance Monitoring (0.0 to 1.0)
# 0.1 = 10% of transactions are sent to Sentry
SENTRY_TRACES_SAMPLE_RATE=0.1
SENTRY_PROFILES_SAMPLE_RATE=0.1

# Prometheus Metrics
PROMETHEUS_METRICS_ENABLED=True
METRICS_ENABLED=False
METRICS_PORT=9090
METRICS_PATH=/metrics

# Health Check Configuration
HEALTH_CHECK_INCLUDE_DETAILS=True
HEALTH_CHECK_DB_TIMEOUT_SECONDS=5

# ====================
# GRAPHQL
# ====================
# Enable/disable GraphQL endpoint (default: disabled)
# When enabled, provides GraphQL API at /graphql
GRAPHQL_ENABLED=true

# ============================================================================
# GOOGLE CLOUD / EARTH ENGINE
GOOGLE_CLOUD_PROJECT=astral-archive-482008-g2

# OAuth 2.0 Client (for user-facing auth)
GOOGLE_OAUTH_CLIENT_ID=your-client-id.apps.googleusercontent.com
GOOGLE_OAUTH_CLIENT_SECRET=your-client-secret

# Service Account (for server-to-server auth - RECOMMENDED)
# Path to the service account JSON key file
GOOGLE_APPLICATION_CREDENTIALS=./floodingnaque-service-account.json

# Service Account Email
GOOGLE_SERVICE_ACCOUNT_EMAIL=floodingnaque@astral-archive-482008-g2.iam.gserviceaccount.com

# Earth Engine Settings
EARTHENGINE_ENABLED=True
EARTHENGINE_PROJECT=astral-archive-482008-g2
EARTHENGINE_CACHE_DIR=data/earthengine_cache

# BigQuery Settings (for historical weather data)
BIGQUERY_ENABLED=True
BIGQUERY_DATASET=weather_data

# Data Sources Toggle
GPM_PRECIPITATION_ENABLED=True
CHIRPS_PRECIPITATION_ENABLED=True
ERA5_REANALYSIS_ENABLED=True

# ====================
# WorldTides API Configuration
# ====================
# Get your API key from: https://www.worldtides.info/settings
# Provides tidal data for coastal flood prediction
WORLDTIDES_API_KEY=api_key_here
WORLDTIDES_ENABLED=True
WORLDTIDES_DATUM=MSL
WORLDTIDES_CACHE_TTL_SECONDS=1800

# ====================
# Datadog APM (Linux/Docker Only)
# ====================
# Datadog provides comprehensive APM, metrics, and log aggregation
# NOTE: Disabled by default for thesis/low-traffic projects to save costs
# Re-enable when scaling to production or when cost is not a concern
#
# Installation (Linux/Docker only - see requirements.txt):
#   pip install ddtrace  # Automatically installed on non-Windows platforms
#
# Usage:
#   ddtrace-run gunicorn -b 0.0.0.0:5000 main:app
#
# Configuration:
DD_APM_ENABLED=False
DD_LOGS_ENABLED=False
DD_AGENT_HOST=localhost
DD_TRACE_AGENT_PORT=8126
DD_SERVICE=floodingnaque-backend
DD_ENV=production
DD_VERSION=2.0.0
# Sampling rate (0.0-1.0): Lower values reduce costs
DD_TRACE_SAMPLE_RATE=0.1
# Enable/disable specific integrations
DD_TRACE_FLASK_ENABLED=True
DD_TRACE_SQLALCHEMY_ENABLED=True
DD_TRACE_REQUESTS_ENABLED=True
DD_TRACE_REDIS_ENABLED=True
# For profiling (requires additional setup)
DD_PROFILING_ENABLED=False
